% Created 2019-01-10 Thu 17:04
% Intended LaTeX compiler: pdflatex
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
\usepackage{grffile}
\usepackage{longtable}
\usepackage{wrapfig}
\usepackage{rotating}
\usepackage[normalem]{ulem}
\usepackage{amsmath}
\usepackage{textcomp}
\usepackage{amssymb}
\usepackage{capt-of}
\usepackage{hyperref}
\usepackage[margin=1in]{geometry}
\usepackage[inkscapelatex=false, inkscapepath=svgsubpath]{svg}
\usepackage{booktabs}
\usepackage{placeins}
\usepackage{subcaption}

\renewcommand*{\floatpagefraction}{.7}
\renewcommand*{\dblfloatpagefraction}{.3}
\author{arnold braker}
\date{\today}
\title{Notes On Improving raster scanning and CS scanning}
\hypersetup{
 pdfauthor={arnold braker},
 pdftitle={Notes On Improving raster scanning and CS scanning},
 pdfkeywords={},
 pdfsubject={},
 pdfcreator={Emacs 25.1.1 (Org mode 9.1.4)}, 
 pdflang={English}}
\begin{document}

\maketitle
\tableofcontents
\section{Outline}
I claim that I have fulfilled my Comps obligation to cut CS Time in half. I did not say that I would improve it to an order of magnitude faster than potential raster scanning, only that I would do the CS scan in 40 seconds. I have achieved that.  Here is how I did it.

\begin{itemize}
\item Dramatically reduce the disturbances impinging on the $z$-axis. Those disturbances in part severely limited the the achievable $z$-axis control bandwidth, because increased BW just amplified those disturbances.
\item Compensate for the $z$-piezo bending mode via inversion. Also, compensate for the varying DC-gains between cantilevers, so that PI gain largely stays unchanged. This is automated, and done as pre-step to imaging (or for the day).
  \begin{itemize}
  \item show bode plots with several KI gains with and w/o Dinv.
  \item Show raster row signal with and w/o Dinv.
  \end{itemize}
\item Forget about lifting the cantilever far enough to actually break contact. Rather, only pull away far enough that the deflection signal stays low, even while we run across the surface.
\item $\mu$-path connections.
\item The damage metric, and why SSIM/PSNR suck.
\item Improved post-processing. 
  \begin{itemize}
  \item Aligning images via cross-correlation.
  \item Dynamic detrending.
  \item TV-denoising.
  \end{itemize}
\end{itemize}

\section{Z-axis disturbance Reduction}
Dramatically reduce the disturbances impinging on the $z$-axis. Those disturbances in part severely limited the the achievable $z$-axis control bandwidth, because increased BW just amplified those disturbances.

\subsection{Paste in what I've already written.}
Compensate for the $z$-piezo bending mode via inversion. Also, compensate for the varying DC-gains between cantilevers, so that PI gain largely stays unchanged. This is automated, and done as pre-step to imaging (or for the day).
  
  \begin{itemize}
  \item show bode plots with several KI gains with and w/o Dinv.
  \item Show raster row signal with and w/o Dinv.
  \end{itemize}


\section{Z-axis Control}
With the disturbances attenuated, we can now increase the $z$-axis bandwidth. The main challenge to increasing the bandwidth is the bending mode resonance at 215 Hz, which can be seen in the FRF from $u_Z$ to $d_{fl}$ in Fig.~\ref{fig:z_control} (blue curve). We address this by inverting the complex pole-zero pair at 215 Hz. There are multiple advantages to this method: first an inverse compensator still makes sense in the forward loop if the feedback path is broken and we are applying control in open-loop, a situation that can occur during parts of the CS cycle. Second, as we discuss further below, the inverse compensator requires no tuning as it is defined only by a fit to the measured frequency response and this can be almost completely automated. Using this method, I am able to achieve a closed-loop bandwidth of about 350 Hz.

% In principle, we should be able to push the bandwidth beyond that mode, but because the cancellation is imperfect and the mode appears to be non-linear, we keep the roll-off below so that the closed loop bandwidth is about ~Hz.
\begin{figure}
  \centering
  \includesvg[scale=0.9]{figures/z_control_design.svg}
  \caption{}
  \label{fig:z_control}
\end{figure}
Thus, the entire $z$-axis controller consists of a PI controller, $D_I(z)$ cascaded with the resonance inversion, $D^{-1}(z)$. For comparison, the red curve in Fig.~\ref{fig:z_control} shows the closed-loop FRF without the inverse compensator.
\begin{figure}
  \centering
  \includegraphics[scale=0.5]{figures/AFM_loop_Dinv-crop.pdf}
  \caption{Block diagram of the $Z$-axis control loop with the inverse compensator.}
  \label{fig:afm_bd_dinv}
\end{figure}
The addition of $D^{-1}(z)$ changes the correct signal to use to represent the sample surface. Let 
If we take the output of the entire compensator, $u_Z=D_ID^{-1}e_z$, then we will get a lot of wiggles in the sample surface. Rather, we need to take only the output of the integrator to represent the surface, as shown in Fig.~\ref{fig:afm_bd_dinv}.


One challenge to this approach is the system gain and resonance at 215~Hz changes not only day-to-day but also (and especially) when changing cantilevers. This is illustrated in Fig.~\ref{fig:z_evolution}, which shows the FRF near the mode for two different cantilevers across many days.

\begin{figure}
  \centering
  \includesvg[scale=1]{figures/z_cant_evolution.svg}
  \caption{}
  \label{fig:z_evolution}
\end{figure}
To deal with this, I incorporated a (mostly) automated system ID routine into the imaging software. This routine first obtains an FRF of the mode (the same set of points as in Fig.~\ref{fig:z_evolution}) and then first a 2-pole 2-zero model to the mode and finally passes that data into the imaging package. One of the benefits of this approach is that the different cantilever gains become normalized to unity, which makes re-tuning the control system after changing tips far easier to non-existent.

\subsection{Caveats to Inversion}
It seems that inversion favors raster scanning over CS. The issue, I believe and need to demonstrate more fully, is that the nature of the resonance can shift depending on the setpoint and especially the input magnitude (for sinusoidal inputs). During CS, we ask the system to operate over a wider range, and thus these non-linearties play a larger role. This is evident in Fig XX, where we have connected multiple $\mu$-paths together. What we see is oscillations after the engagement and in the first scan, but which decay after the first hole. Thus, the inversion is more than capable when operating in the limited range around which it was identified, but performance degrades during the approach, which is away from the point at which it was identified.


It is \emph{possible} that the oscillations are from the XY plane being transmitted via sample tilt to the cantilever. I do not think this is the case, because the slowest resonance in the $XY$-plane is like 350~Hz and these oscillations are dominated by 215~Hz.

\section{Cantilever does not need to disengage.} Forget about lifting the cantilever far enough to actually break contact. Rather, only pull away far enough that the deflection signal stays low, even while we run across the surface.

Why? Ultimately the concern is damage to the sample and tip, which is directly caused by the force imparted by the cantilever probe. The deflection signal, though calibrated (meaning it does not directly lead to a value in $kN$), is proportional to that force, because it is proportional to how much the cantilever is bent. I.e., a large (towards $+\infty$) deflection implies the spring force of the cantilever is pushing hard into the sample, thereby inducing damage. In a typical imaging scenario, we choose a deflection setpoint, say $-0.3$~[v], and have in some way decided that this is sufficient, all else being equal, to not damage things too much.

Thus, rather than completely dis-engaging the tip from the surface, I propose to simply move the setpoint towards $-\infty$ during the $XY$-move. If this new setpoint is sufficiently far away, the deflection signal will stay below the scanning setpoint, which we had decided was a ''safe'' value. One challenge here is that about a quarter of the time, the new setpoint far enough away that the probe may snap away from the sample surface anyway. If the move-setpoint more negative than the free value of the deflection, then, because the sensor is saturated, the integrator will wind up and saturate the actuator. To deal with this, we need an anti-windup protection during the transition part the CS cycle.

\section{$\mu$-path connections}
In general, it should be possible to reduce the CS imaging time by optimizing the order of the $\mu$-path scans. This is related to the traveling salesman problem, but a key difference is that the locations of arrival and depature from a given ``city'' are different. A related idea is to connect multiple $\mu$-paths together. For example, if the $\mu$-paths are one pixel apart, it does not make sense to go through an entire CS cycle of lifting the tip away from the sample surface, moving in the $XY$-plane, and re-engaging the tip, waiting for it to settle and finally starting the next scan. Rather, for $\mu$-paths within a certain threshold, it makes sense to leave the tip on the surface and simply move to the next location at a speed not exceeding the given scan rate. 

\section{Improved post-processing}
\subsection{Aligning images via cross-correlation.}
\begin{equation}
  Z_{k,\ell} = \sum_{m=0}^{M-1}\sum_{n=0}^{N-1} X_{m-k, n-\ell}Y_{m-k, n-\ell}
\end{equation}
\subsection{Dynamic detrending}


\subsection{ TV-denoising}
Some of the noise in image recovered via BPDN can be reduced by solving the optimization problem
\begin{equation}
  \min_{U} ||\nabla_xU||_1 + ||\nabla_yU||_1 + \mu||F - U||_2 \label{eqn:breg1}
\end{equation}
where $F$ is the image produced by the BPDN optimization. 
This is basically the same, or at least heavily inspired by Yufan's approach. The difference is that (1) I do this for both $\nabla_x$ and $\nabla_y$, where he only does it for $\nabla_y$ and (2) that he does it in a single optimization where I do it in two optimizations. I will not compare the two approaches.

There is some evidence that this is a bad idea from a metrological point of view because \eqref{eqn:breg1} does not appear to unit DC-gain. In other words, if $\mu$ is too small, the humps in the image get squashed, yet we need $\mu$ large to eliminate the noise. Therefore, I will show metrics for both.

\section{The Metrics and Setting Expectations}
In an effort to provide some sort of quantitative comparison between images, we consider three metrics. The first two compare a master image to some corrupted version

The first is the Structural Similarity Index (SSIM) \cite{wang_image_2004}. The second is the Peak Signal to Noise Ratio (PSNR) \cite{Luo_nano_2015}. Both metrics compare a master image to some distorted version, and have been primarily developed by the image processing community in an effort to provide a quantitative measure of image corruption, e.g., when comparing compression algorithms.

Define the master image as $X$ and a reconstruction as $Y$, with each having $p\times p=n$ pixels. Stack the columns of each into the vectors $x, y\in\mathbb{R}^{n^2}$. Let $L$ be the dynamic range of the master image $x$. Then the PSNR is given by
\begin{equation*}
  \text{PSNR}(x,y) = 10\log_{10}\frac{L^2}
  {\sqrt{\frac{1}{p^2} \sum_{i=1}^{n}( x_{i} - y_{i})^2}}.
\end{equation*}
The goal of the SSIM is to compare two image's structure, luminescence, and contrast and is built up from the means ($\mu_x$ and $\mu_y$), standard deviations ($\sigma_x$ and $\sigma_y$), and covariance ($\sigma_{xy}$) of the image vectors $x$ and $y$.  
The variation of the SSIM used in this paper is defined as
\begin{equation*}
  \text{SSIM}(x,y) = \frac{(2\mu_x\mu_y + C_1)(2\sigma_{xy}+C_2)}
  {(\mu_x^2 + \mu_y^2 + C_1)(\sigma_x^2 + \sigma_y^2 + C_2)}
\end{equation*}
where the constants $C_1$ and $C_2$ are regularizing constants to prevent singularity if, e.g., $\mu_x=\mu_y=0$. We use the default values suggested in \cite{wang_image_2004} of $C_1=(0.01L)^2$ and ${C_2=(0.03L)^2}$, where again, $L$ is the dynamic range.

Both metrics have been used before to compare \emph{simulations} of CS reconstruction in the context of AFM \cite{oxvig_structure_2017, Luo_nano_2015}. We believe, however, that the numbers presented here in Table~\ref{tab:metrics} should be interpreted with some caution as it remains somewhat of an open question of how to best compare \emph{experimental} images from AFM.



However, in AFM imaging, pure image quality is not the only concern, particularly for delicate samples. In addition, we need a figure of merit for how much damage was done to specimen during the imaging process. While this is not really a concern while imaging a hard calibration grating, it plays a prominent role biological samples. In general, damage occurs while scanning into an uphill region, which results in a positive $z$ error signal. The actual damage done by a given $z_e$ will depend on the spring constant of the cantilever and the softness of the specimen. Nonetheless, we can use the positive deviations of $z_e$ as a relative measure of damage between different control gains or scanning methods, for a given cantiliver and specimen. This motivates a metric we term the relative damage index (RDI), which we define as 
\begin{equation}
  \text{RDI} = \frac{1}{NT_s}\sum_{k=0}^{N-1} \left(\max(0,~z_{dk}-r_{\textrm{scan}})\right)^2 \label{eqn:RDI}
\end{equation}
where, $N$ is the total number of samples in a given scan, $z_{dk}$ is the deflection signal at sample $k$ and $r_{\textrm{scan}})$ is the scanning setpoint. The RDI is basically the power in positive deflection.
By excluding negative values of the deflection, we do not penalize the CS algorithm while it is re-engaging with the specimen \emph{unless} it overshoots the scanning setpoint, which we do want to penalize. 

What kind of numbers are reasonably to expect from these metrics? I.e., how do we know when to stop optimizing? In the following two subsections,  we consider two things: (i) quality metrics for a batch of slow raster scans all taken at the same slow speed of 0.5~Hz. and (ii) CS simulations, where we sample an actual raster image (the master) with the same sampling pattern we implement 

\begin{figure}
  \begin{subfigure}{.48\textwidth}
    \includesvg[width=1\textwidth]{figures/baseline_errors_noalign.svg}
    \caption{}
    \label{fig:rast_unaligned}
  \end{subfigure}
  \begin{subfigure}{.48\textwidth}
    \includesvg[width=1\textwidth]{figures/baseline_errors_aligned.svg}
      \caption{}
    \label{fig:baseline_errors_aligned}
  \end{subfigure}
  \caption{Errors between a ''master'' and 6 different raster scans, all taken sequentially and at a scan rate of 0.5 Hz. (a) Errors without alignment (b) The same data as above in (a) but aligned via cross correlation.}
  \label{fig:baseline_errors}
\end{figure}

\subsection{SSIM and PSNR show wide variance}
There are issues, especially in our experimental setup, with comparing images with SSIM and PSNR. Recall that actuation in the $XY$-plane is done via the nPoint C300 and the $XY$ axes of the original Agilent piezo tube are unused. Unfortunately, this means that drift occurs in the XY plane. We take two measures to counteract this. First, we allow the piezo tube ample time to warm up before starting an imaging session, so that the major part of the drift transient has died out. Second, we compute the PSNR and SSIM metrics on sub-slices of the actual images, which are then aligned via two dimensional cross correlation \cite{mw_xcorr2}. 

However, even with these techniques, images taken across the same region and at the same slow scan rate exhibit substantial variation in their relative quality metrics. This is illustrated in Fig.~\ref{fig:baseline_errors}, which shows a comparison of 7 raster scans taken at 0.5~Hz. The scans were taken sequentially across the same 5 micron by 5 micron region of the CS20NG sample grating. Using the first image as the master, and compute the PSNR and SSIM metrics for the remaining six images after aligning the master with a subslice (25 pixels on each side). Fig.~\ref{fig:baseline_errors} shows the error between the aligned master and the comparee. Notably, the PSNR ranges from nearly a high of 20 to a low of about 9; the SSIM ranges from 0.89 to 0.46. Even if we exclude the 7th image (bottom right pane) which shows a lot of drift in the $Y$-direction in the bottom third of the image, there is still substantial spread in the remaining metrics. 

Therefore, all these numbers should be taken with a large grain of salt.



\subsection{CS Simulations}
In this section, we consider a raster scan taken at 0.5~Hz. We then consider CS simulations using the resulting image by sub sampling the image with a $\mu$-path mask and doing a CS image reconstruction. The goal is learn what kind of quality might be expected and to help is understand how much of the degradation is due to the nature of CS and how much is due to limitations in our experimental setup (e.g., transients from the tip descent or inconsistency stemming from like a $z$ position sensor.).


\begin{figure}[h!]
  \begin{subfigure}{1\textwidth}
      \includesvg[width=.87\textwidth]{figures/cs_sim_0p5Hz_raster.svg}
  \caption{}
  \label{fig:cs_sim_0p5}
  \end{subfigure}
  \begin{subfigure}{1\textwidth}
    \includesvg[width=.87\textwidth]{figures/cs_sim_0p5Hz_raster_from1Hz.svg}
    \caption{}
    \label{fig:cs_sim_raster_0p5_1p0}
  \end{subfigure}
  \caption{CS simulations. (a) The top left panel is a 0.5 Hz raster scan. The rest of the panels were sub-sampled with a mu-path mask with the indicated sampling fraction and reconstructed with BPDN. (b) subsampled images taken from a 1~Hz raster scan and compared to the same 0.5~Hz scan. Comparing the 0.5~Hz raster scan to the 1.0~Hz scan yields PSNR=17.91 and SSIM=0.57.}
  \label{fig:cs_sim_against_raster}
\end{figure}

Comparing the 0.5~Hz raster scan to the 1.0~Hz scan yields PSNR=17.91 and SSIM=0.57.

We need to characterize how much of the poor quality is due to CS itself and how much is due to the experiment. To do this, we take a raster image of a 5 micron square at 0.5~Hz. We the simulate CS reconstructions by sub sampling that image with several $\mu$-path masks and reconstruct the image with BPDN. These results are shown in Fig.~\ref{fig:cs_sim_against_raster}. We compute the PSNR and SSIM metrics, which are shown in table XX.

We note several things. The halos show up here. The reconstructed images are noisy and the PSNR and SSIM numbers are quite low. The CS experts probably say this is due to the $\mu$-paths introducing too much coherence in the measurements or that the image isn't sparse enough for the number of measurements we took. The TV denoising helps the SSIM metric but doesn't do much for the PSNR.

\section{Experimental Results}
We did some scans. Here are some pictures. There are also some tables so this all appears scientific.

\bibliographystyle{IEEEtran}
\bibliography{/home/arnold/bib_pdf/main_bibliography}

\end{document}
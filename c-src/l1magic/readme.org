#+TITLE: l1C: A (very) partial c port of l1-magic
#+SETUPFILE: ~/.emacs.d/org-templates/level-0.org
#+PROPERTY: TEMPLATE page_math
#+PROPERTY: URL projects/mpc_j.html 
#+PROPERTY: SAVE_AS projects/mpc_j.html
#+OPTIONS: tex:t
#+STARTUP: latexpreview
#+TODO: TODO(t) WAITING(w@/!) | DONE(d@/!) CANCELED(c@/!) STARTED(s@/!) DEFERRED(ef@/!)
#+STARTUP: fold
#+STARTUP: lognotestate t
#+SEQ_TODO: TODO STARTED WAITING DELEGATED APPT | DONE DEFERRED CANCELLED



* Introduction
This code is basically a c port of portions of the [[https://statweb.stanford.edu/~candes/l1magic/][l1-magic toolbox]], with a few modifications. Currently, the only ported capability is to solve the optimization problem

\begin{align}
\min_{x} ||x||_{1}  \quad \text{s.t.} \quad ||Ax -b||_{2} < \epsilon
\end{align}
which is converted to the quadratically constrained linear program
\begin{align}
\min_{x,u} \quad \mathbb{1}^{T}u \quad &\text{s.t.} \\
 x - u &< 0 \\
 -x - u &< 0\\
 ||Ax -b||_{2} &< \epsilon
\end{align}
The inequality constraints are converted to barrier functions. The ~l1-magic~ manual does a really good job walking through all of this. 

Thus, in the terms of ~l1-magic~ this code replaces the functionality of ~l1qc_log_barrier.m~ and its associated ~l1qc_newton.m~ and ~cgsolve.m~). Currently, the code *only* operates in large-scale mode, which means that the transformation $Ax$ (and the adjoint, $A^Ty$) are performed by user supplied functions, rather than an explicit matrix-vector multiplication). If your interest is in "small-scale" mode, modifying the code for that scenario should be relatively straigforward.

* Performance
So far, using ~l1C~ gives me a speed increase of between 2 and 5 times faster compared to the original matlab code, depending on which computer I run it on.


* Building
Typing ~make~ in the root directoy will print a summary of availible targets and options. To summarize, the availible targets are:
- ~make test~, will build the unit tests and output a file in the ~lqc~ root directoy called ~test_l1qc~.
- ~make test_data~, will launch matlab as a terminal application and build the test data, which is derived from the original ~l1-magic~ Matlab implementation.
- ~make mex~, will build a mex style interface for Matlab. Note that, due to the requirement of supplying user-defined functions which evaluate $Ax$ and $A^{T}y$, this target is best thought of as an example. You will want to modify the mex file for your particular application. 
  - Currently, I do not link into shared libraries when building the mex file, but rather link the archive (which fortunately both OpenBlas, FFTW and MKL supply) so that the mex function is completely self contained. This eliminates all the problems with Matlab not respecting your ~LD_LIBRARY_PATH~. If you know how to make that work consistently, let me know!
  - The makefile assumes your Matlab is installed at /usr/local/MATLAB/R2018b. If this is not the case, modify the makefile variable ~MATLAB_ROOT~.
- ~make lib~, which will build a shared .so style library. This is probably only useful if we want to use ~l1c~ with other, non-matlab code. See above.
- ~make ar~, will build an archive file (.a). 
- Options. Currently, you can supply two different options.
  - ~USE_MKL=[1|0]~. If 1, the library will be build using MKL. Otherwise, it will be built with OpenBlas and FFTW. Default is 0. If ~USE_MKL=1~, the makefile will assume that your MKL is installed in 
    ~/opt/intel/compilers_and_libraries/linux/mkl~. If this is not the case, you can either comment out the ~MKLROOT~ variable in the makefile and export the proper location before running make or modify the variable in the makefile.
  - ~DEBUG=[1|0]~. If 1, build with debugging symbols and turn optimizations off. Default is 0. To debug with optimizations on, please edit the makefile, (e.g., add ~-g~ to ~CFLAGS~).
** Dependencies
*** Core Code
The core code with examples requires a linear algebra library and an FFT library capable of performing a DCT. The best performance I have seen is by compiling with Intel's [[https://software.intel.com/en-us/mkl][Math Kernel Library (MKL)]]. I beleive most of the increased performance is due to their faster DCT routines. 

This is, however, unecessary and ~l1C~ can be built with [[https://github.com/xianyi/OpenBLAS][OpenBlas]] and [[http://fftw.org/][FFTW]]. 

Note that in general, the only use of the DCT is for the (user defined) $A*x$ and $A^{T}*y$ transformations. If your use case uses a different sort of transformation, then an FFT library is unecessary. If this is the case, then you can simply adapt the mex interface file to your needs. Running the tests would require you to modify them and supply a different transformation, and to eliminate the tests for dct.c and dct_mkl.c.

*** Unit Tests
The unit tests depend on two additional libraries. Data for the unit tests is generated in Matlab for the more complicated functions. Data for simple routines is embedded within the test function itself (see, e.g., ~TEST_vcl.c~ and ~test_logsum()~ in ~TEST_l1qc_newton.c~).
1. [[https://github.com/libcheck/check][libcheck]] is the unit testing framework. The makefile assumes the headers for check are located in 
 ~/usr/local/include~ and that libcheck.so is located in ~/usr/local/lib~.
2. To load data generated by matlab, we use [[https://github.com/DaveGamble/cJSON][cJSON]].

It is highly recommended to install these dependencies and run the tests.
** Building the test data

1. ~build_log_barrier_test_data.m~: this scripts builds data to test the outer log-barrier iterations. JSON files are saved into test_data with the name ~sprintf('lb_test_data_iter_%d.json', lb_iter)~.
2. ~~build_newton_init_data.m~ Builds a small set of data to check that we compute the number of log-barrier iterations and tau parameter properly. Used in ~test_newton_init()~.
* Usage
As a user, the primary function you need to worry about is
#+BEGIN_SRC c 

/*l1qc_newton.h */
LBResult l1qc_newton(int N, double *x, double *u, int M, double *b, 
                     NewtParams params, AxFuns Ax_funs;
#+END_SRC

- ~int N~. The length of ~x~ and ~u~.
- ~double *x~. On entry, this should be an array of doubles length N, allocated on a 64-byte boundary (see below). On exit, x contains the result of the optimization.
- ~double *u~ On entry, this should contain an array with length N. On exit, it will contain the auxilary u (See above about the conversion from an l1 optimization to a linear program).
- ~int M~. The length b.
- ~double *b~. On entry, contains the 'measured data' (see above). In general, we expect M <N.
- ~NewtParams params~ is a struct containing parameters (e.g., tolerances and iteration number bounds). Will be described fully below.
- ~AxFuns Ax_funs~ is a struct containing pointers to the functions which perform the transformations.


*Important*: The array inputs of doubles (*x, *u, *b) to ~l1qc_newton~ must be aligned on a 64-byte boundary, otherwise segfaults may occur. To faciliate this, you may use the functions. 

#+BEGIN_SRC c 
/*l1c_common.h */
void* malloc_double(N);
void* free_double(N);
#+END_SRC
The function ~malloc_double(N)~ will allocate memory for ~N~ doubles, aligned on a 64-byte boundary and ~free_double~ will free it.


The data structures are defined as
#+BEGIN_SRC c
//l1qc_newton.h
typedef struct LBResult{
  double l1;                // Final value of functional, ||x||_1
  int    total_newton_iter; // Total number of newton iterations.
  int    status;            // 0 if completed with no errors, 1 otherwise

}LBResult;

typedef struct NewtParams{
  double epsilon;
  double tau;
  double mu;
  double newton_tol;
  int newton_max_iter;
  int lbiter;
  double lbtol;
  int verbose;
  CgParams cg_params;

}NewtParams;

typedef struct AxFuns {
  void(*Ax)(double *x, double *y);
  void(*Aty)(double *y, double *x);
  void(*AtAx)(double *x, double *z);
}AxFuns;
#+END_SRC

The struct ~AxFuns~ contains pointers to your user-defined functions which compute $Ax$ and $A^{T}y$. For an example, see the mex-interface file ~l1qc_mex.c~ (in ~interfaces/~) and either ~dct.c~ or ~dct_mkl.c~. Note that although the mex interface looks long and complicated, almost all of this is boiler-plate parsing of Matlab's input to the function. The amount of code to modify for a different set of transform functions is only a few lines.

* Modifications from the ~l1-magic~ algorithm
I have made a few changes (improvements?) to the original ~~l1-magic~ algorithm, both pertaining to the line search. I believe these changes add issues with numerical, rather than mathematical, problems. As the ~l1-magic~ authors note, in the later stages of the optimziation, numerical difficulties arise and the line search can fail. These modifications help to push that point into the future, enabling more iterations.

1. In the original code, I noticed that at some point, the data become complex when it should have been purely real. One of the places where this occures is in the code which computes the maximum step-size which still satisfies the constraints (i.e., lines XX in the original code). In particular, the code which computes the largest number $s$ such such that, for $x_{k+1}= x_{k} + sd_{x_k}$, $||Ax_{k+1}-b||<\epsilon$ still holds. To do this, we expand into a scalar equation quadratic in $s$
   \begin{align}
   ||A(x+sd_{x})-b||^{2} - \epsilon^{2} &=0 \\
   s^{2}(d_x^{T}A^{T}Ad_x) + 2r^{T}Ad_x + r^{T}r - \epsilon^{2} &= 0\\
   \end{align}

   where $r = Ax - b$. Although the roots should always be real, due to either computing $d_{x}$ with insufficient accuracy (which accomplished via conjugate gradient) or otherwise, the roots become complex in the later stages. In matlab, the promation to a complex type happens silently and we start optimizing complex data, which is undersirable. In c, the ~sqrt~ operation simply returns NaN, which is also undersirable. When this happens, the modification is to set $s=1$ and let the line search deal with. This will work fine in c because taking the log of a negative number results in NaN. In Matlab, we need something like ~log(max(0, a))~.

2. The goal of the line-search is to find (approximitaly) the largest step-size $s$ such that
   \begin{equation}
   f(z + sd_{z}) < f(z) + \alpha s \nabla f\cdot d_{z}
   \end{equation}
   In the original code, the functional $f(z)$ is only evaluated explicitly at the start of each log-barrier iteration and the value of $f(z_{i})$ is updated from derived values, e.g., $r_{k+1}= r_{k} + sAd_{x}$. Mathematically, this is fine. Numerically, it is problematic because after enough iterations the explicit value of $f(z_{k})$ becomes infinite (due to the barrier functions) even though the putative value is finite. Thus, although it is less efficient, this code evaluates the functional explicitly at each iteration of the line-search and this value is then passed to the next Newton iteration.

* To-Dos
**  Make ~MATLAB_ROOT~ and ~MKLROOT~ set-able from the commandline, or figure out a way to detect them.
